## 实验四 电影评论情感分类实验报告

##### 202228013329025 张喜玥

### 一、 实验目的

1. 进一步加深对卷积神经网络基本原理的理解。
2. 掌握卷积神经网络处理文本的各项技术。
3. 掌握文本分类模型 Text-CNN 的架构和原理。

### 二、 实验要求

1. 任选一个深度学习框架建立 Text-CNN 模型。
2. 实现对中文电影评论的情感分类，实现测试准确率在 83%以上。
3. 也可采用 LSTM 实现，实现测试准确率高于卷积神经网络。

### 三、实验数据集及环境

1.数据集：

1）训练集。包含2W条左右中文电影评论，其中正负向评论各1W条左右。

2）验证集。包含6K条左右中文电影评论，其中正负向评论各3K条左右。 

3）测试集。包含360条左右中文电影评论，其中正负向评论各180条左右。 

4）预训练词向量。中文维基百科词向量word2vec。

2.实验框架：基于`Pytorch`框架构建网络

3.实验环境：Google colab平台

### 四、网络架构及实验参数

#### 网络架构

本实验试用两种网络进行情感分类，首先使用TextCNN网络，网络结构如下图，

<img src="../../../../../../Library/Application%20Support/typora-user-images/%E6%88%AA%E5%B1%8F2023-06-18%2014.00.31.png" alt="截屏2023-06-18 14.00.31" style="zoom:50%;" />

<div align = "center">图1 TextCNN网络架构</div>

双向LSTM网络结构如下：

<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20230618133340690.png" alt="image-20230618133340690" style="zoom:50%;" />

<div align = "center">图2 BiLSTM网络架构</div>

#### 实验参数

```python
BATCH_SIZE = 16  # 批次大小
EPOCHS = 10  # 迭代轮数
WINDOWS_SIZE = [2, 3, 4]  # 滑动窗口大小
EMBEDDING_DIM = 50
FEATURE_SIZE = 256  # 特征大小
N_CLASS = 2  # 标签类数
LEARNING_RATE = 0.005
MAX_SEN_LEN = 50
```

采用`Adam`优化器和交叉熵损失函数`CrossEntropyLoss`。

### 五、代码说明

1.数据集预处理

（1）原始数据文件内容包括情感标签和分词后的电影评论，读取txt文件，将标签存储在 `label`变量中，将分词后的句子转换成词向量的形式存储在 `content`列表中，每个词转换成word2id中的索引进行存储，设置最大长度为50，不足50的句子补充0，长度超过50的进行截断操作

```python
def load_data(path, word2id):
    contents_list, labels_list = [], []
    with open(path, encoding='utf-8') as f:
        for line in f.readlines():
            sentences = line.strip().split()
            if sentences == []:
                continue
            label = int(sentences[0])
            content = [word2id.get(sen, 0) for sen in sentences[1:]]
            content = content[:MAX_SEN_LEN]
            if len(content) < MAX_SEN_LEN:
                content += [word2id['_PAD_']] * (MAX_SEN_LEN - len(content))
            labels_list.append(label)
            contents_list.append(content)
    contents = np.array(contents_list)
    labels = np.array(labels_list)

    labels_onehot = np.array([[0, 0]] * len(labels_list))
    for idx, val in enumerate(labels):
        if val == '0':
            labels_onehot[idx][0] = 1
        else:
            labels_onehot[idx][1] = 1

    return contents, labels, labels_onehot
```

（2）构建词汇表，将数据集中的文本构造成{word:id}的形式，最后生成的字典长度为数据集中所有已出现词汇的总数

```python
def build_word2id(save_to_path=None):
    """
    :param save_to_path: path to save word2id
    :return: word2id dictionary {word: id}
    """
    word2id = {'_PAD_': 0}
    path = ['./Dataset/train.txt', './Dataset/validation.txt']

    for _path in path:
        with open(_path, encoding='utf-8') as f:
            for line in f.readlines():
                sp = line.strip().split()
                for word in sp[1:]:
                    if word not in word2id.keys():
                        word2id[word] = len(word2id)
    if save_to_path:
        with open(save_to_path, 'w', encoding='utf-8') as f:
            for w in word2id:
                f.write(w + '\t')
                f.write(str(word2id[w]))
                f.write('\n')

    return word2id
```

（3）基于预训练好的word2vec构建训练语料中所含词语的word2vec

fname: 预训练的 word2vec.

word2id: 语料文本中包含的词汇集.

返回语料文本中词汇集对应的 word2vec 向量{id: word2vec}.

```python
def build_word2vec(fname, word2id, save_to_path=None):
    n_words = max(word2id.values()) + 1
    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)
    wordid_vecs = np.array(np.random.uniform(-1., 1., [n_words, model.vector_size]))
    for word in word2id.keys():
        try:
            wordid_vecs[word2id[word]] = model[word]
        except KeyError:
            pass
    if save_to_path:
        with open(save_to_path, 'w', encoding='utf-8') as f:
            for vec in wordid_vecs:
                vec = [str(w) for w in vec]
                f.write(' '.join(vec))
                f.write('\n')
    return wordid_vecs
```

3.构建网络模型，TextCNN网络模型

```python
class TextCNN(nn.Module):
    def __init__(self, word2vec, vocab_size, embedding_dim, feature_size, windows_size, max_len, n_class):
        super(TextCNN, self).__init__()
        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
        self.embed.weight.data.copy_(torch.from_numpy(word2vec))
        self.conv1 = nn.ModuleList([
            nn.Sequential(nn.Conv1d(in_channels=embedding_dim, out_channels=feature_size, kernel_size=h),
                          nn.LeakyReLU(),
                          nn.MaxPool1d(kernel_size=max_len - h + 1),
                          )
            for h in windows_size]
        )
        self.dropout = nn.Dropout(p=0.5)
        self.fc1 = nn.Linear(in_features=feature_size * len(windows_size), out_features=n_class)

    def forward(self, x):
        x = self.embed(x)  
        x = x.permute(0, 2, 1)
        x = [conv(x) for conv in self.conv1]
        x = torch.cat(x, 1)
        x = x.view(-1, x.size(1)) 
        x = self.dropout(x)
        x = self.fc1(x)
        return x
```

BiLSTM网络模型

```python
class BiLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):
        super(BiLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
        # LSTM
        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True,
                            bidirectional=True)
        self.dropout = nn.Dropout(p=0.5)
        self.fc = nn.Linear(in_features=hidden_size * 2, out_features=num_classes)

    def forward(self, x):
        x = self.embed(x)  
        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)
        out, (h_n, c_n) = self.lstm(x, (h0, c0))
        output_fw = h_n[-2, :, :]  
        output_bw = h_n[-1, :, :]  
        out = torch.concat([output_fw, output_bw], dim=1)  
        x = self.fc(out)
        return x
```

4.加载数据集、设置损失函数、优化器等相关参数

```python
train_contents, train_labels, _ = load_data('./Dataset/train.txt', word2id)
val_contents, val_labels, _ = load_data('./Dataset/validation.txt', word2id)
test_contents, test_labels, _ = load_data('./Dataset/test.txt', word2id)
contents = np.vstack([train_contents, val_contents])
labels = np.concatenate([train_labels, val_labels])
train_dataloader = DataLoader(MyDataSet(contents, labels), batch_size=BATCH_SIZE, shuffle=True)
test_dataloader = DataLoader(MyDataSet(test_contents, test_labels), batch_size=BATCH_SIZE, shuffle=True)

train_len = len(contents)
test_len = len(test_contents)
# 构建模型
vocab_size = len(word2id)
model = TextCNN(word2vec=word2vec, vocab_size=vocab_size, embedding_dim=EMBEDDING_DIM, windows_size=WINDOWS_SIZE,
                max_len=MAX_SEN_LEN, feature_size=FEATURE_SIZE, n_class=N_CLASS).to(device)
# model = BiLSTM(vocab_size=vocab_size, embedding_dim=EMBEDDING_DIM, hidden_size=MAX_SEN_LEN,
#                num_layers=2, num_classes=N_CLASS).to(device)
# print(model)
optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=0.0005)
# 模型训练
train(model, train_dataloader, test_dataloader, optimizer, train_len, test_len)
# 模型保存
torch.save(model.state_dict(), './TextCNN.pkl')
```

5.训练模型

```python
def train(model, train_loader, test_loader, optimizer, train_len, test_len):
    train_loss_list, train_acc_list, test_loss_list, test_acc_list = [], [], [], []
    for epoch in range(EPOCHS):
        model.train()
        train_correct, test_correct, batch_num, train_loss, test_loss = 0, 0, 0, 0, 0
        for i, (input, label) in enumerate(train_loader):
            batch_num += 1
            input, label = input.to(device), label.to(device)
            output = model(input)
            loss = loss_func(output, label)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            pred = torch.max(output, 1)[1].cpu().numpy()
            label = label.cpu().numpy()
            correct = (pred == label).sum()
            train_correct += correct
            train_loss += loss.item()
        train_loss_list.append(train_loss / BATCH_SIZE)

```

6.测试模型

```python
model.eval()
outs, labels = [], []
with torch.no_grad():
  for ii, (input, label) in enumerate(test_loader):
    input, label = input.to(device), label.to(device)
    output = model(input)
    loss = loss_func(output, label)
    pred = torch.max(output, 1)[1].cpu().numpy()
    label = label.cpu().numpy()
    correct = (pred == label).sum()
    test_correct += correct
    test_loss += loss.item()
    outs.append(output)
    labels.append(label)
    test_loss_list.append(test_loss / BATCH_SIZE)
    train_acc = train_correct / train_len
    test_acc = test_correct / test_len
    train_acc_list.append(train_acc)
    test_acc_list.append(test_acc)
    print("Epoch:", epoch + 1,
          "train_loss:{:.5f}, test_loss:{:.5f}, train_acc:{:.2f}%, test_acc:{:.2f}%".format(train_loss / (i + 1),test_loss / (ii + 1),train_acc * 100,test_acc * 100))
```

6.计算准确率、精确率、F1-score、召回率

```python
def get_accuracy(model, datas, labels):
    out = model(torch.LongTensor(np.array(datas)).to(device))
    predictions = torch.max(input=out, dim=1)[1]  # 最大值的索引
    y_predict = predictions.to('cpu').data.numpy()
    y_true = labels
    accuracy = accuracy_score(y_true, y_predict)  # 准确率
    precision = precision_score(y_true, y_predict)
    recall = recall_score(y_true, y_predict)
    f1 = f1_score(y_true, y_predict)
    print('准确率：', accuracy, '\n正确率：', precision, '\n召回率：', recall, '\nF1值：', f1)
```

### 六、实验结果

使用TextCNN训练结果：

在训练10个epoch后对测试集的准确率达到84%左右。实验结果如下图所示

<img src="../../../../../../Library/Application%20Support/typora-user-images/%E6%88%AA%E5%B1%8F2023-06-18%2013.58.11.png" alt="截屏2023-06-18 13.58.11" style="zoom:50%;" />

<div align = "center">图2 TextCNN实验结果</div>

使用BiLSTM训练结果：

在训练10个epoch之后，测试集准确率可以达到85%左右，较TextCNN稍有提升。

<img src="../../../../../../Library/Application%20Support/typora-user-images/%E6%88%AA%E5%B1%8F2023-06-18%2013.59.10.png" alt="截屏2023-06-18 13.59.10" style="zoom:50%;" />

<div align = "center">图2 BiLSTM实验结果</div>




## 实验三 自动写诗实验报告

##### 202228013329025 张喜玥

### 一、 实验目的

1. 理解和掌握循环神经网络概念及在深度学习框架中的实现。
2. 掌握使用深度学习框架进行文本生成任务的基本流程:如数据读取、构造网络、训练和预测等。

### 二、 实验要求

1. 基于Python语言和任意一种深度学习框架，完成数据读取、网络设计、网络构建、模型训练和模型测试等过程，最终实现一个可以自动写诗的程序。
2. 随意给出首句，如给定“湖光秋月两相和”，输出模型续写的诗句。也可以根据自己的兴趣，进一步实现写藏头诗(不做要求)。要求输出的诗句尽可能地满足汉语语法和表达习惯。实验提供预处理后的唐诗数据集，包含 57580 首唐诗(在课程网站下载)，也可以使用其他唐诗数据集。
3. 按规定时间在课程网站提交实验报告、代码以及PPT。

### 三、实验数据集及环境

1.数据集：本实验使用实验数据是预处理过的数据集 `tang.npz`，含有 57580 首唐诗，每首诗限定在 125 词。包含三个部分:

(1)  data: 诗词数据，将诗词中的字转化为其在字典中的序号表示。

(2)  ix2word: 序号到字的映射

(3)  word2ix: 字到序号的映射

唐诗开始用`<START>`标志，索引8291；结束用`<EOP>`标志，索引8290；长度不足的用`</s>`填充，索引8292

2.实验框架：`Python 3.7.10`，基于`Pytorch`框架构建网络

3.实验环境：华为ModelArts云平台，GPU: 1*P100(16GB)|CPU: 8核 64GB

### 四、网络架构及实验参数

#### 网络架构

本实验的网络模型是基于字符级的语言模型，数据的标签为诗句中每一个汉字的下一个汉字。所以使用embedding+LSTM网络结构构造模型

embedding层用于生成词向量，输入维度：`se1_len,batch_size,embedding_dim`

LSTM层用于处理数据

全连接层使用两个线性层进行处理

```python
nn.Sequential(nn.Linear(self.lstm_outdim, 2048),
                        nn.Tanh(),
                        nn.Linear(2048, vocab_size))
```

#### 诗句生成

诗句生成逻辑是先输入一个汉字，并根据当前的输入和隐藏状态得到一个汉字的预测输出。然后，检查这个输出是否在给定的句子范围内。如果是，那么这个输出将被摒弃，并使用给定句子的下一个字作为输入，直到输出的汉字超过了给定的句子范围。如果输出的汉字超过了给定的句子范围，那么预测的输出句子将作为下一个输入，并继续生成下一个汉字。

在这个过程中，模型的隐藏状态会被更新，并影响当前的输出。当前的输出不仅取决于当前的输入，还取决于之前的输入和隐藏状态。因此，在每个时间步中，模型的隐藏状态会被传递到下一个时间步，以影响下一个汉字的预测输出。即模型中的`hidden`张量是一直在模型中传递的，直到生成的全部诗句。

#### 实验参数

```python
EMBEDDING_DIM = 256
HIDDEN_DIM = 512
LR = 0.001
EPOCHS = 20
LSTM_LAYER = 3
BATCH_SIZE = 16
MAX_GEN_LEN = 125
```

采用`Adam`优化器和交叉熵损失函数`CrossEntropyLoss`。

### 五、代码说明

1.数据集预处理

（1）首先加载数据集：从npz文件中读取数据和字典对

```python
def prepareData():
    datas = np.load("tang.npz", allow_pickle=True)
    data = datas['data']
    ix2word = datas['ix2word'].item()
    word2ix = datas['word2ix'].item()
    data = preprocess(data)
    dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    return dataloader, ix2word, word2ix
```

（2）对每句话进行处理：文件中原始句子将空格补充在诗句前面，此处更改空格的位置，使长度不足的诗句在后面补空格

```python
def preprocess(sentences):
    new_sentences = []
    for sentence in sentences:
        new_sentence = [token for token in sentence if token != 8292]
        if len(new_sentence) < MAX_GEN_LEN:
            new_sentence.extend([8292] * (MAX_GEN_LEN - len(new_sentence)))
        else:
            new_sentence = new_sentence[:125]
        new_sentences.append(new_sentence)
    sentences = np.array(new_sentences)
    sentences = torch.tensor(sentences, dtype=torch.long)
    return sentences
```

2.构建网络模型，包括词向量层、LSTM层、全连接层

- `vocab_size`：词表大小，即每首诗中所有不同汉字的数量。
- `embedding_dim`：词向量维度，即将每个汉字转换为一个向量的维度。

输入首先通过词向量层进行处理，得到一个`(seq_len, batch_size, embedding_dim)`的张量。然后该张量作为输入传递给LSTM层，LSTM层会对每个时间步的输入进行处理，并输出一个`(seq_len, batch_size, hidden_dim)`的张量作为隐藏状态。然后通过全连接层序列进行降维。

```python
class PoetryModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(PoetryModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = LSTM_LAYER
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=self.num_layers)
        self.fc = nn.Sequential(nn.Linear(self.hidden_dim, 2048),
                                nn.Tanh(),
                                nn.Linear(2048, vocab_size))
        self.dropout = nn.Dropout(0.6)

    def forward(self, input, hidden=None):
        seq_len, batch_size = input.size()
        #print(input.shape)
        if hidden is None:
            h_0 = input.data.new(self.num_layers, batch_size, self.hidden_dim).fill_(0).float()
            c_0 = input.data.new(self.num_layers, batch_size, self.hidden_dim).fill_(0).float()
        else:
            h_0, c_0 = hidden
        embeds = self.embeddings(input)
        output, hidden = self.lstm(embeds, (h_0, c_0))
        output = self.dropout(output)
        output = self.fc(output.view(seq_len * batch_size, -1))
        return output, hidden
```

3.训练模型

定义模型、设置优化器和损失函数、获取模型输出、计算误差、误差反向传播

```python
def train(epochs, poem_loader, word2ix):
    model = PoetryModel(len(word2ix), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)
    model.train()
    model.to(device) 
    optimizer = optim.Adam(model.parameters(), lr=LR)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        for batch_idx, data in enumerate(poem_loader):
            data = data.long().transpose(1,0).contiguous()
            data = data.to(device)
            optimizer.zero_grad()
            input, target = data[:-1, :], data[1:, :]
            output, _ = model(input)
            loss = criterion(output, target.view(-1))
            loss.backward()
            optimizer.step()
            if batch_idx % 900 == 0:
                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(epoch + 1, batch_idx * len(data[1]), len(poem_loader.dataset), 100. * batch_idx / len(poem_loader), loss.item()))
                
        torch.save(model.state_dict(),"model{}_.pth".format(epoch))
```

4.生成诗句

- 首先将输入的首句转换为一个列表，并存储在`results`变量中，并计算输入首句的长度。将首词设置为`<START>`
- 通过循环生成诗句中的每个字。如果当前正在处理输入的首句，则直接将首句中的汉字作为模型的输入，否则获取模型输出概率最高的汉字的数字下标。并通过 `ix2word` 转换为汉字
- 如果生成的汉字是结束标记`<EOP>`，则将其从结果列表中删除，并结束唐诗的生成。

```python
def generate(model, start_words, ix2word, word2ix):
    model.eval()
    results = list(start_words)
    start_word_len = len(start_words)

    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long()
    input = input.to(device)
    h = None
    model = model.to(device)
    model.eval()

    for i in range(125):
        output, h = model(input, h)
        if i < start_word_len:
            w = results[i]
            input = input.data.new([word2ix[w]]).view(1, 1)
        else:
            top_index = output.data[0].topk(1)[1][0].item()
            w = ix2word[top_index]
            results.append(w)
            input = input.data.new([top_index]).view(1, 1)
        if w == '<EOP>':
            del results[-1]
            break

    return results
```

5.生成藏头诗

```python
def gen_acrostic(model, start_words, ix2word, word2ix):
    result = []
    start_words_len = len(start_words)
    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long()
    # 指示已经生成了几句藏头诗
    index = 0
    pre_word = '<START>'
    hidden = None
    model = model.to(device)
    model.eval()
    input = input.to(device)

    for i in range(125):
        output, hidden = model(input, hidden)
        top_index = output.data[0].topk(1)[1][0].item()
        w = ix2word[top_index]
        if pre_word in {'。', '，', '?', '！', '<START>'}:
            if index == start_words_len:
                break
            else:
                w = start_words[index]
                index += 1
                input = (input.data.new([word2ix[w]])).view(1, 1)
        else:
            input = (input.data.new([top_index])).view(1, 1)
        result.append(w)
        pre_word = w
    return result
```

### 六、实验结果

本实验在训练20个epoch后loss下降至1.5左右

```
Train Epoch: 1 [0/57580 (0%)]	Loss: 9.023953
Train Epoch: 1 [14400/57580 (25%)]	Loss: 2.221473
Train Epoch: 1 [28800/57580 (50%)]	Loss: 2.619899
Train Epoch: 1 [43200/57580 (75%)]	Loss: 2.423241
Train Epoch: 2 [0/57580 (0%)]	Loss: 2.966218
Train Epoch: 2 [14400/57580 (25%)]	Loss: 2.033438
Train Epoch: 2 [28800/57580 (50%)]	Loss: 1.863774
Train Epoch: 2 [43200/57580 (75%)]	Loss: 1.772649
Train Epoch: 3 [0/57580 (0%)]	Loss: 1.921006
Train Epoch: 3 [14400/57580 (25%)]	Loss: 2.107684
Train Epoch: 3 [28800/57580 (50%)]	Loss: 1.725447
Train Epoch: 3 [43200/57580 (75%)]	Loss: 1.997398
Train Epoch: 4 [0/57580 (0%)]	Loss: 1.791882
Train Epoch: 4 [14400/57580 (25%)]	Loss: 2.683145
Train Epoch: 4 [28800/57580 (50%)]	Loss: 2.039846
Train Epoch: 4 [43200/57580 (75%)]	Loss: 1.792837
Train Epoch: 5 [0/57580 (0%)]	Loss: 1.617785
Train Epoch: 5 [14400/57580 (25%)]	Loss: 2.257158
Train Epoch: 5 [28800/57580 (50%)]	Loss: 2.035940
Train Epoch: 5 [43200/57580 (75%)]	Loss: 2.171076
Train Epoch: 6 [0/57580 (0%)]	Loss: 1.419853
Train Epoch: 6 [14400/57580 (25%)]	Loss: 1.951049
Train Epoch: 6 [28800/57580 (50%)]	Loss: 1.879415
Train Epoch: 6 [43200/57580 (75%)]	Loss: 1.792953
Train Epoch: 7 [0/57580 (0%)]	Loss: 1.503780
Train Epoch: 7 [14400/57580 (25%)]	Loss: 1.698478
Train Epoch: 7 [28800/57580 (50%)]	Loss: 1.672707
Train Epoch: 7 [43200/57580 (75%)]	Loss: 2.051444
Train Epoch: 8 [0/57580 (0%)]	Loss: 1.554273
Train Epoch: 8 [14400/57580 (25%)]	Loss: 1.795549
Train Epoch: 8 [28800/57580 (50%)]	Loss: 1.651695
Train Epoch: 8 [43200/57580 (75%)]	Loss: 1.873264
Train Epoch: 9 [0/57580 (0%)]	Loss: 1.675153
Train Epoch: 9 [14400/57580 (25%)]	Loss: 1.694709
Train Epoch: 9 [28800/57580 (50%)]	Loss: 1.671458
Train Epoch: 9 [43200/57580 (75%)]	Loss: 1.752009
Train Epoch: 10 [0/57580 (0%)]	Loss: 1.568793
Train Epoch: 10 [14400/57580 (25%)]	Loss: 1.620910
Train Epoch: 10 [28800/57580 (50%)]	Loss: 1.673024
Train Epoch: 10 [43200/57580 (75%)]	Loss: 1.689510
Train Epoch: 11 [0/57580 (0%)]	Loss: 1.811221
Train Epoch: 11 [14400/57580 (25%)]	Loss: 1.291192
Train Epoch: 11 [28800/57580 (50%)]	Loss: 1.832296
Train Epoch: 11 [43200/57580 (75%)]	Loss: 1.748741
Train Epoch: 12 [0/57580 (0%)]	Loss: 1.925065
Train Epoch: 12 [14400/57580 (25%)]	Loss: 1.119470
Train Epoch: 12 [28800/57580 (50%)]	Loss: 2.232790
Train Epoch: 12 [43200/57580 (75%)]	Loss: 1.827150
Train Epoch: 13 [0/57580 (0%)]	Loss: 1.776992
Train Epoch: 13 [14400/57580 (25%)]	Loss: 1.241391
Train Epoch: 13 [28800/57580 (50%)]	Loss: 1.314360
Train Epoch: 13 [43200/57580 (75%)]	Loss: 2.205727
Train Epoch: 14 [0/57580 (0%)]	Loss: 1.586706
Train Epoch: 14 [14400/57580 (25%)]	Loss: 1.804242
Train Epoch: 14 [28800/57580 (50%)]	Loss: 1.502965
Train Epoch: 14 [43200/57580 (75%)]	Loss: 2.012017
Train Epoch: 15 [0/57580 (0%)]	Loss: 1.630675
Train Epoch: 15 [14400/57580 (25%)]	Loss: 1.674470
Train Epoch: 15 [28800/57580 (50%)]	Loss: 1.979553
Train Epoch: 15 [43200/57580 (75%)]	Loss: 2.065588
Train Epoch: 16 [0/57580 (0%)]	Loss: 1.558938
Train Epoch: 16 [14400/57580 (25%)]	Loss: 1.442625
Train Epoch: 16 [28800/57580 (50%)]	Loss: 1.418451
Train Epoch: 16 [43200/57580 (75%)]	Loss: 1.657187
Train Epoch: 17 [0/57580 (0%)]	Loss: 1.619367
Train Epoch: 17 [14400/57580 (25%)]	Loss: 1.450153
Train Epoch: 17 [28800/57580 (50%)]	Loss: 1.294634
Train Epoch: 17 [43200/57580 (75%)]	Loss: 1.719152
Train Epoch: 18 [0/57580 (0%)]	Loss: 1.490737
Train Epoch: 18 [14400/57580 (25%)]	Loss: 1.677017
Train Epoch: 18 [28800/57580 (50%)]	Loss: 1.465044
Train Epoch: 18 [43200/57580 (75%)]	Loss: 1.628020
Train Epoch: 19 [0/57580 (0%)]	Loss: 1.621380
Train Epoch: 19 [14400/57580 (25%)]	Loss: 1.478449
Train Epoch: 19 [28800/57580 (50%)]	Loss: 1.620635
Train Epoch: 19 [43200/57580 (75%)]	Loss: 1.254071
Train Epoch: 20 [0/57580 (0%)]	Loss: 1.387534
Train Epoch: 20 [14400/57580 (25%)]	Loss: 1.382168
Train Epoch: 20 [28800/57580 (50%)]	Loss: 1.556970
Train Epoch: 20 [43200/57580 (75%)]	Loss: 1.715397
```

生成诗句测试：

开头输入：花开花落几番时

藏头诗输入：深度学习

使用epoch1生成的模型测试：

<img src="../../../../../../Library/Application%20Support/typora-user-images/image-20230617230109123.png" alt="image-20230617230109123" style="zoom:50%;" />

使用epoch10生成的模型测试：

<img src="../../../../../../Library/Application%20Support/typora-user-images/%E6%88%AA%E5%B1%8F2023-06-17%2023.03.27.png" alt="截屏2023-06-17 23.03.27" style="zoom:50%;" />

使用epoch20生成的模型测试：

<img src="../../../../../../Library/Application%20Support/typora-user-images/%E6%88%AA%E5%B1%8F2023-06-17%2023.04.09.png" alt="截屏2023-06-17 23.04.09" style="zoom:50%;" />

<div align = "center">图1 实验结果</div>

### 七、网络改进

在网络中添加一层lstm进行训练，迭代15个epoch训练结果如下，loss值相比上述网络有所下降。

此时网络结构如下：

```python
class PoetryModel2(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(PoetryModel2, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = LSTM_LAYER
        self.lstm_outdim = LSTM_OUTDIM
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.lstm1 = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=self.num_layers)
        self.lstm2 = nn.LSTM(self.hidden_dim, self.lstm_outdim, num_layers=self.num_layers, batch_first=True)
        self.fc = nn.Sequential(nn.Linear(self.lstm_outdim, 2048),
                                nn.Tanh(),
                                nn.Linear(2048, vocab_size))
        self.dropout = nn.Dropout(0.6)

    def forward(self, input, hidden1=None, hidden2=None):
        seq_len, batch_size = input.size()
        if hidden1 is None or hidden2 is None:
            h_0 = input.data.new(self.num_layers, batch_size, self.hidden_dim).fill_(0).float()
            c_0 = input.data.new(self.num_layers, batch_size, self.hidden_dim).fill_(0).float()
            h_1 = input.data.new(self.num_layers, seq_len, self.lstm_outdim).fill_(0).float()
            c_1 = input.data.new(self.num_layers, seq_len, self.lstm_outdim).fill_(0).float()
        else:
            h_0, c_0 = hidden1
            h_1, c_1 = hidden2
        embeds = self.embeddings(input)
        output, hidden1 = self.lstm1(embeds, (h_0, c_0))
        output, hidden2 = self.lstm2(output, (h_1, c_1))
        output = self.dropout(output)
        output = self.fc(output.reshape(seq_len * batch_size, -1))
        return output, hidden1, hidden2
```

训练结果如下：

```
Train Epoch: 1 [0/57580 (0%)]	Loss: 9.043023
Train Epoch: 1 [14400/57580 (25%)]	Loss: 2.559448
Train Epoch: 1 [28800/57580 (50%)]	Loss: 2.812332
Train Epoch: 1 [43200/57580 (75%)]	Loss: 2.441596
Train Epoch: 2 [0/57580 (0%)]	Loss: 2.172956
Train Epoch: 2 [14400/57580 (25%)]	Loss: 2.289348
Train Epoch: 2 [28800/57580 (50%)]	Loss: 2.265936
Train Epoch: 2 [43200/57580 (75%)]	Loss: 2.342879
Train Epoch: 3 [0/57580 (0%)]	Loss: 2.391577
Train Epoch: 3 [14400/57580 (25%)]	Loss: 2.250081
Train Epoch: 3 [28800/57580 (50%)]	Loss: 2.381099
Train Epoch: 3 [43200/57580 (75%)]	Loss: 1.727954
Train Epoch: 4 [0/57580 (0%)]	Loss: 2.094553
Train Epoch: 4 [14400/57580 (25%)]	Loss: 1.765476
Train Epoch: 4 [28800/57580 (50%)]	Loss: 1.897615
Train Epoch: 4 [43200/57580 (75%)]	Loss: 2.102326
Train Epoch: 5 [0/57580 (0%)]	Loss: 1.865850
Train Epoch: 5 [14400/57580 (25%)]	Loss: 1.348168
Train Epoch: 5 [28800/57580 (50%)]	Loss: 1.519472
Train Epoch: 5 [43200/57580 (75%)]	Loss: 2.213043
Train Epoch: 6 [0/57580 (0%)]	Loss: 1.825005
Train Epoch: 6 [14400/57580 (25%)]	Loss: 2.146451
Train Epoch: 6 [28800/57580 (50%)]	Loss: 1.953478
Train Epoch: 6 [43200/57580 (75%)]	Loss: 1.709889
Train Epoch: 7 [0/57580 (0%)]	Loss: 1.934232
Train Epoch: 7 [14400/57580 (25%)]	Loss: 1.540944
Train Epoch: 7 [28800/57580 (50%)]	Loss: 1.723153
Train Epoch: 7 [43200/57580 (75%)]	Loss: 1.903077
Train Epoch: 8 [0/57580 (0%)]	Loss: 1.297468
Train Epoch: 8 [14400/57580 (25%)]	Loss: 1.618726
Train Epoch: 8 [28800/57580 (50%)]	Loss: 1.350372
Train Epoch: 8 [43200/57580 (75%)]	Loss: 1.662134
Train Epoch: 9 [0/57580 (0%)]	Loss: 1.855022
Train Epoch: 9 [14400/57580 (25%)]	Loss: 1.412588
Train Epoch: 9 [28800/57580 (50%)]	Loss: 1.592857
Train Epoch: 9 [43200/57580 (75%)]	Loss: 2.228262
Train Epoch: 10 [0/57580 (0%)]	Loss: 1.395069
Train Epoch: 10 [14400/57580 (25%)]	Loss: 1.609208
Train Epoch: 10 [28800/57580 (50%)]	Loss: 1.906870
Train Epoch: 10 [43200/57580 (75%)]	Loss: 1.729615
Train Epoch: 11 [0/57580 (0%)]	Loss: 2.401567
Train Epoch: 11 [14400/57580 (25%)]	Loss: 1.554679
Train Epoch: 11 [28800/57580 (50%)]	Loss: 2.005925
Train Epoch: 11 [43200/57580 (75%)]	Loss: 1.687335
Train Epoch: 12 [0/57580 (0%)]	Loss: 1.342209
Train Epoch: 12 [14400/57580 (25%)]	Loss: 1.638798
Train Epoch: 12 [28800/57580 (50%)]	Loss: 1.524010
Train Epoch: 12 [43200/57580 (75%)]	Loss: 1.329655
Train Epoch: 13 [0/57580 (0%)]	Loss: 1.245436
Train Epoch: 13 [14400/57580 (25%)]	Loss: 1.385828
Train Epoch: 13 [28800/57580 (50%)]	Loss: 1.676270
Train Epoch: 13 [43200/57580 (75%)]	Loss: 1.594991
Train Epoch: 14 [0/57580 (0%)]	Loss: 1.904134
Train Epoch: 14 [14400/57580 (25%)]	Loss: 1.394085
Train Epoch: 14 [28800/57580 (50%)]	Loss: 1.424864
Train Epoch: 14 [43200/57580 (75%)]	Loss: 1.316742
Train Epoch: 15 [0/57580 (0%)]	Loss: 1.483809
Train Epoch: 15 [14400/57580 (25%)]	Loss: 1.070218
Train Epoch: 15 [28800/57580 (50%)]	Loss: 1.465840
Train Epoch: 15 [43200/57580 (75%)]	Loss: 1.423411
```

诗句生成结果：

<img src="../../../../../../Library/Application%20Support/typora-user-images/%E6%88%AA%E5%B1%8F2023-06-17%2023.13.50.png" alt="截屏2023-06-17 23.13.50" style="zoom:50%;" />

可见，相比一层lstm来说，双层lstm生成的诗句更加优美，增加训练次数，可以得到更加丰富的诗句。
